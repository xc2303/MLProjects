{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdj-Tdpp8aTZ"
   },
   "source": [
    "# Portfolio Description Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUa4zNabZ3zk"
   },
   "source": [
    "\n",
    "Prompt: Your project descriptions might include a description of the **dataset** you analyzed for the project, the types of **predictive models** you fit to the data, and **any other salient details** you may wish to add.  You should also store each of the reports you created this semester s in a subfolder of your Github repo.  Then provide a link to each file in your write up.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OOHi6Ag7ZsY0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpMSf36waHSo"
   },
   "source": [
    "# Assignment 1 \n",
    "Github Link: https://github.com/xc2303/MLProjects/blob/e1ff436caec113f29ac8991a3a43b12273a4dfdf/Assignment1/Assignment1.ipynb\n",
    "\n",
    "Assignment 1 is about predicting world happiness level based on the UN World Happiness Data. \n",
    "\n",
    "## About the dataset: \n",
    "The World Happiness Report is a survey of the state of global happiness conducted by the UN, which ranks 155 countries by their happiness levels. Leading experts across fields – economics, psychology, survey analysis, national statistics, health, public policy and more – describe how measurements of well-being metrics can be used effectively to assess the progress of nations in terms of their happiness. \n",
    "\n",
    "The Y (target variable) is Happiness_level, which is the respondents' happiness level. It consists of 5 unique values: Very High', 'High', 'Average', 'Low', 'Very Low. \n",
    "\n",
    "The columns (feature variables X) following the Happiness_level estimate the extent to which each of the factors – country of region, GDP per capita, social support, healthy life expectacy, freedom to make life choices, generosity, perception of corruption, country name, region, subregion  – contribute to making life more happier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WdsffrRUesuP"
   },
   "source": [
    "## Data exploration:\n",
    "I first dropped columns including: name, Country or region, region, sub-region, which are duplicated features. \n",
    "Second, I numerically encode target variable from 0 to 4, which indicate 'very low' to 'very high'.\n",
    "\n",
    "From the correlation graph, there seems to be positive relationship between GDP per capita, social support, healthy life expectancy (features) and happiness level (target variable).\n",
    "\n",
    "Freedom to make life choices, generosity and percentage of corruption (features) do not seem to be correlated with happiness level (target variable).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbX38EX2exKZ"
   },
   "source": [
    "## Feature Selection & Pre-processing: \n",
    "\n",
    "I used a recursive feature selection method (RFECV) with cross validation to automatically select best features from a linear regression model, and examine them. \n",
    "\n",
    "I found each feature is positively correlated with happiness level. we can also interpret the strength of the coefficient: Freedom to make life choices have the greatest effect with coefficient of 1.89; the smallest effect is perception of corruption, which has a coefficient of 0.326. \n",
    "\n",
    "I then preprocessed data using Column Transformer, more specifically, using SimpleImputer and StandardScalar for numeric features and SimpleImputer and OneHotEncoder for non-numeric categorical feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EoXTDsolox1t"
   },
   "source": [
    "## Models & Analysis: \n",
    "\n",
    "After that I fit 3 models into the training and test dataset: XGBoost (hyper-parameter: learning rate = 0.8), RandomForest (hyper-parameter: n_estimators: 200), and SVC.\n",
    "\n",
    "For the test accuracy score:\n",
    " Test accuracy for xgboost classifier is 0.42; for support vector classifier is 0.38; for random forest classifier is 0.423. SVC.\n",
    " XGBoost/RF has the highest test accuracy score among all three models. After I have taken a closer look at their parameters, I also observed that XGBoost has hyperparameters of: colsample_bylevel=1,\n",
    "colsample_bynode=1, \n",
    "colsample_bytree=1, \n",
    "gamma=0\n",
    "learning_rate=0.8, among many others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OIeN-FEpDxG"
   },
   "source": [
    "The takeaway and additional thoughts from the assignment 1 is that: looking back all these models don't seem to have very high prediction score, and lower than 50%. I might improve the model using neural network/ deep learning models (e.g. multi-layer perceptron, add dense layers, add regularization) in the future for better prediction results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qfcDMWj_hShi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onuqAurVpDSW"
   },
   "source": [
    "# Assignment 2\n",
    "Github link: https://github.com/xc2303/MLProjects/blob/e1ff436caec113f29ac8991a3a43b12273a4dfdf/Assignment2/COVID_19_radiography%20-%20XC.ipynb\n",
    "\n",
    "Assignment 2 is about predicting x-rays images that demonstrate Covid Postivity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mRBDIWgiG5j"
   },
   "source": [
    "## About the dataset: \n",
    "\n",
    "X-ray imaging is a tool that helps in the COVID-19 diagnosis. Through utilization of artificial intelligence (AI), more and more patients can be more easily detected of COVID-19 from their chest X-ray images. The dataset was obtained from the paper 'Can AI help in screening Viral and COVID-19 pneumonia?' by\n",
    "Muhammad E. H. Chowdhury et. al. The analysis for the assignment could be useful for someone in the emergency room, to take image of the person's X-ray chest and get result immediately. If the model is good, it can increase accuracy of prediction of whether the person has COVID or not.\n",
    "\n",
    "The dataset contains 3 categories: covid, normal and pneumonia images, and each has 1200, 1341 and 1345 images respectively (total 3886 images).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zR4hl_rmiHd5"
   },
   "source": [
    "## Preprocessing: \n",
    "\n",
    "\n",
    "Preprocessesed image data with shape 192 x 192 x 3 using a preprocessor function. In addition, resize the image and change image colors using cv2 for modeling. So that the min value of every image is 0 and max value of every image is 1. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U5Q593Q6iLJN"
   },
   "source": [
    "## Visualization: \n",
    "I visualized a few sample images that are COVID positive and Normal through pre-process and import each images and display in a grid. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69GYx5vJiK_-"
   },
   "source": [
    "## Model & Analysis \n",
    "\n",
    "I ran 3 models for the image dataset. \n",
    "\n",
    "The 1st model I used is densely Connected Feed Forward NN for Baseline. The model has 4 layers: dense layer that takes the original image of (192,192,3) dimension (relu activation), another dense layer(relu activation), and flatten layer and finally the last dense layer (softmax activation). \n",
    "\n",
    "The 2nd model I used is Convolutional Neural Network. It has 10 layers. 2 convolution layers, MaxPooling2D layer, followed by another 2 convolutional lyaers, followed by another MaxPooling2D layer, and then another set of conv2D layer, flatten and dense layers. \n",
    "This model also uses early stopping and reduce learning rate when val_accuracy fails to improve after 3 epochs. \n",
    "\n",
    "The 3rd model I used is transfer learning model - I first used VGG16 to get a base model. Then I added 3 conv2D layers, a GlobalAveragePooling2D layer and Dense layer. The total params of this model is 29,138,243.Then I fit VGG16 model with frozen imagent weights and new input/output layer shapes.\n",
    "\n",
    "Finally, I evaluated all the models and the accuracy of 1st dense model is 0.90, CNN model is 95.9%, and transfer learning model is 95.6%. \n",
    "\n",
    "Compared with the top model, which has an accuracy of 98.7%. The maximum depth of the model is 22, and contains 33593155 number of parameters. It uses Adam optimizer, versus mine uses SGD optimizer, has 4 layers and 7082307 parameters. To improve my model, I can add more dense layers in the future. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mSSbTOsNQlGU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6gwylwzsiGDs"
   },
   "source": [
    "# Assignment 3\n",
    "\n",
    "Github Link: https://github.com/xc2303/MLProjects/blob/5bd1710339814d0f31f994b54cb088dfbd73fe6a/Assignment3/Covid_Tweet_Misinformation_Classification_Mini_Hackathon_submission(1).ipynb\n",
    "\n",
    "Assignment 3 is about predicting whether COVID tweets contain real and false information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0lnhvMPZB_p"
   },
   "source": [
    "## About the dataset:\n",
    "\n",
    "Even as scientific understanding of the COVID-19 pandemic develops, there is an emergence of unsubstantiated rumors, which is  false or misleading and may lead to significant risk of harm (such as increased exposure to the virus, or adverse effects on public health systems). Therefore, some Tweets are labeled false to prevent misleading information about COVID-19 from spreading on Twitter. \n",
    "\n",
    "The training dataset contains 6420 rows and 2 columns. The first column is tweet information in text format; the second column is a real/fake label column. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-6_ks4BaIPn"
   },
   "source": [
    "## Preprocessing: \n",
    "\n",
    "I defined the preprocessor to tokenize the words into the same length, with the maxlen of 40 and max words to pass on 10,000. And then transformed the label data to 0 /1 for prediction purpose. \n",
    "\n",
    "I split the training and test into 2 groups, with training contains 6420 rows and test dataset contains 2140 rows. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIyWApo-aHoP"
   },
   "source": [
    "## Models & Analysis:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QGRly7riaHfJ"
   },
   "source": [
    "The 1st model I used is simple RNN model. THe model contains 3 layers. Embedding, Simple RNN and Dense layer (sigmoid activcation). The prediction accuracy of the model is around 90%.\n",
    "\n",
    "Tbe 2nd model I used is stacked RNN model. I had 6 layers for this model: Embedding, LSTM, SimpleRNN, SimpleRNN, SimpleRNN, and Dense layer with sigmoid activation. The prediction accuracy of the model is around 91%.\n",
    "\n",
    "Tbe 3rd model I had is LSTM model. I had 3 layers for this model: Embedding, LSTM, and Dense layer with softmax activation. The prediction accuracy of the model is around 92%.\n",
    "\n",
    "\n",
    "Tbe 4th model I had is Bidirectional LSTM model. I had 3 layers for this model: Embedding, Bidirectional LSTM, and Dense layer with sigmoid activation. The prediction accuracy of the model is around 92%.\n",
    "\n",
    "Tbe 5th model I had is Bidirectional LSTM model with dropout layer. I had 3 layers for this model: Embedding, LSTM (with dropout = 0.2), and Dense layer with sigmoid activation, . The prediction accuracy of the model is around 93%.\n",
    "\n",
    "Tbe 6th model I had is Conv1D model. I had 6 layers for this model: Embedding, Conv1D with relu activation, MaxPooling5D, Conv1D with relu activation, GlobalMaxPooling1D and Dense layer. The prediction accuracy of the model is around 90% \n",
    "\n",
    "I used rmsprop optimizer and binary crossentropy as the loss function. \n",
    "\n",
    " The difference between the top model (Version 61) and my model (version 62) is that, the accuracy score is much higher with 95.0% accuracy versus mine: 94.2%; and f1_scroe of 94.99% versus mine: 94.19%. The best model uses 5 layers in total: Embedding, Bidirectional, LSTM, and 2 Dense layers. The model contains 1,081,482 number of parameters. My model has 3 layers in total: Embedding, LSTM and Dense. It has 402,690 parameters. For activation and optimizer: the best model used relu, softmax and tanh activation while my model used sigmoid and tanh activation. Both models used RMSprop as optimizer. LSTM is better at prediction text than convolutional models. \n",
    "\n",
    "In the end, I fit my best model to real tweets data and got the prediction probability score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SNRrORGbZsJW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Portfolio Notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
